<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"/>
    <title>News Recommender</title>
    <link rel="stylesheet" href="write-up/tufte.css"/>
    <link rel="stylesheet" href="write-up/latex.css"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
  </head>

  <body>
    <article>
      <h1 id="tufte-css">Dismantling Political Echo Chambers Using Natural Language Processing and Binary Classifiers</h1>
      <p class="subtitle">Nilai Vemula</p>
      <section>
        <p>Echo chambers, or places where citizens are exclusively exposed to information that reinforces their prior beliefs, are becoming more prevalent<label for="sn-kellogg" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-kellogg" class="margin-toggle"/><span class="sidenote"><a href="https://medium.com/thrive-global/why-echo-chambers-are-becoming-louder-and-more-polarizing-44aba2a231e7">Why Echo Chambers Are Becoming Louder</a></span> due to the spread of social media and more relevant due to increased political engagement in the United States. These echo chambers have been criticized for accelerating polarization<label for="sn-axios" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-axios" class="margin-toggle"/><span class="sidenote"><a href="https://www.axios.com/echo-chambers-news-entertainment-272236ca-9097-471c-b1ae-7aaa5ed3d5a7.html">Echo Chambers are Getting Worse</a></span> and are hotspots for spreading misinformation<label for="sn-pubmed" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-pubmed" class="margin-toggle"/><span class="sidenote">See this interesting <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6147442/">study</a> modeling fake news in echo chambers.</span>. The goal of this project is to create a tool that recommends news articles about a given topic with diverse perspectives to encourage users to break out of their bubbles and consider other perspectives. </p>
        
        <p>The hardest part of any machine learning project is finding a good source of annotated data to train a model with. For this, I turned to popular <a href="https://reddit.com">Reddit</a> groups (subreddit) sorted by political leaning. From these groups, I collected the most popular news articles shared in each group. <label for="mn-github" class="margin-toggle">&#8853;</label><input type="checkbox" id="mn-github" class="margin-toggle"/><span class="marginnote">All code for this project can be found on <a href="https://github.com/NilaiVemula/news-recommender">GitHub</a>.</span>The text of these articles was then transformed into a vector space, so that a binary classifier could digest that data and predict the political lean of the subreddit that it was shared in. Finally, I put this ML model to use in a web application. The user can search for a topic, and this application scrapes articles found on <a href="https://news.google.com">Google News</a> and puts them through the model to classify these articles and recommend them to the user. </p>
      </section>

      <section>
        <h2 id="data">Collecting Data</h2>
        <p>The source of my annotated data was in the form of Reddit posts. Forums on Reddit called subreddits have topics that span the spectrum in discussing science to relationships. This is an excellent data choice for this project as these subreddits are essentially echo chambers as nearly all members of the group join to read and engage with material that is supportive of the subreddit's overall topic. I chose 8 of these subreddits with explicit political lean:</p>

        <ul><a href="https://reddit.com/r/Conservative">r/Conservative</a></ul>
        <ul><a href="https://reddit.com/r/Republican">r/Republican</a></ul>
        <ul><a href="https://reddit.com/r/donaldtrump">r/donaldtrump</a></ul>
        <ul><a href="https://reddit.com/r/Anarcho_Capitalism">r/Anarcho_Capitalism</a></ul>
        <ul><a href="https://reddit.com/r/Liberal">r/Liberal</a></ul>
        <ul><a href="https://reddit.com/r/Democrats">r/Democrats</a></ul>
        <ul><a href="https://reddit.com/r/JoeBiden">r/JoeBiden</a></ul>
        <ul><a href="https://reddit.com/r/LateStageCapitalism">r/LateStageCapitalism</a></ul>

        <p>The first four of these subreddits I classified as having a "right-leaning" bias whereas the last four have a "left-leaning" bias. These communities were chosen because they all have a generally large reach (>100k members) and embody the diversity in each of these two categories. Out of the four, one was based off of general ideology (conservative or liberal), one on political party (Republican or Democrat), one on political leader (Donald Trump or Joe Biden), and one representing a more fringe ideology (late stage capitalism or anarcocapitalism). Very popular subreddits such as <a href="https://reddit.com/r/politics">r/politics</a> and <a href="https://reddit.com/r/libertarian">r/libertarian</a> were excluded because they do not have an explicit political lean in the "right"-"left" binary.</p>

        <p>My hypothesis is that news articles that become popular in these subreddits are representative of articles that form of the media diet of people of that group's corresponding political lean. In some cases, this theory breaks down. For example, the most popular post in r/conservative is a <a href="https://www.reddit.com/r/Conservative/comments/jpu2rx/ap_has_called_the_election_for_joe_biden/">news article proclaiming Joe Biden's 2020 Presidential election victory</a>. This article is not particularly informative for categorizing the population of r/conservative, and was likely popular due to the larger Reddit audience "upvoting" the post. For this reason, I selected more than just the top few most popular posts, and I did not weight further analysis on how popular each post was.</p>

        <p>From each of these subreddits, I collected the 1,000 most popular posts (or "top" in Reddit lingo) using a Python library called <a href="https://github.com/praw-dev/praw">praw</a>, which is a wrapper on the Reddit API. I then checked if a post contained a link to a news article, and used the <a href="https://github.com/codelucas/newspaper">newspaper3k</a> library to scrape and parse these articles. The final output of the data collection phase was a <a href="https://github.com/NilaiVemula/news-recommender/tree/main/data">spreadsheet</a> of over 2,000 links to news articles, a cleaned version of their content, their corresponding Reddit post, and some data about that post.</p>

        <p>Overall, 2,321 articles were collected from the 8,000 Reddit posts analyzed, but these articles were not found evenly across the subreddits above. After filtering out articles with less than 50 characters of content and posts with messing data, we were left with 2,037 total articles from the following subreddits:</p>

        <style type="text/css">
            .tg  {border-collapse:collapse;border-spacing:0;}
            .tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
              overflow:hidden;padding:10px 5px;word-break:normal;}
            .tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
              font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
            .tg .tg-baqh{text-align:center;vertical-align:top}
            .tg .tg-amwm{font-weight:bold;text-align:center;vertical-align:top}
            </style>
            <table class="tg">
            <thead>
              <tr>
                <th class="tg-amwm">subreddit</th>
                <th class="tg-amwm">Number of News Articles</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td class="tg-baqh">r/Liberal</td>
                <td class="tg-baqh">850</td>
              </tr>
              <tr>
                <td class="tg-baqh">r/Democrats</td>
                <td class="tg-baqh">386</td>
              </tr>
              <tr>
                <td class="tg-baqh">r/Republican</td>
                <td class="tg-baqh">349</td>
              </tr>
              <tr>
                <td class="tg-baqh">r/Conservative</td>
                <td class="tg-baqh">325</td>
              </tr>
              <tr>
                <td class="tg-baqh">r/JoeBiden</td>
                <td class="tg-baqh">103</td>
              </tr>
              <tr>
                <td class="tg-baqh">r/donaldtrump</td>
                <td class="tg-baqh">14</td>
              </tr>
              <tr>
                <td class="tg-baqh">r/LateStageCapitalism</td>
                <td class="tg-baqh">9</td>
              </tr>
              <tr>
                <td class="tg-baqh">r/Anarcho_Capitalism</td>
                <td class="tg-baqh">1</td>
              </tr>
            </tbody>
            </table>
        
        <p>We have more articles from left-leaning subreddits and more articles from the broader groups. The subreddits focused on candidates and fringe ideologies had few news articles that became popular in the subreddit when shared.</p>

      </section>

      <section>
        <h2 id="preparing">Preparing Our Data</h2>
        <p>In order to use the content of these news articles in a machine learning model, we must first transform this text data into some sort of numeric format. I had heavy use of several Python libraries for this purpose. <a href="https://www.nltk.org/">nltk</a> is a general purpose Natural Language Processing (NLP) package with the ability to organize text data, remove stop words, preform tagging, etc., and <a href="https://radimrehurek.com/gensim/">gensim</a> combines vector-based modeling with topic modeling and document indexing.</p>

        <p>First, I used nltk to tokenize (or separate) the article into individual words; remove stop words such as "a", "the", or punctuation; and compute word frequencies of the most used words in each article. For example, <a href="https://apnews.com/f0e2af5f9f99de9d30dc6b9097121188">this article</a> about President Trump's infamous $750 income tax payment uses the word "president" 12 times, "donald" 3 times, and "trump" 20 times.</p>

        <p>Then, I loaded a <a href="https://code.google.com/archive/p/word2vec/">word2vec model</a> which was pre-trained on a Google News dataset of about 100 billion words. This model fits well with our goal of vectorizing news articles. A <a href="https://en.wikipedia.org/wiki/Word2vec">word2vec</a> model works by creating a 300-dimension vector space. A word is a point in that vector space and is defined as a linear combination of 300 basis vectors. The similarity between two words can be computed using the distance between those two words in the vector space. <label for="sn-cosine" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-cosine" class="margin-toggle"/><span class="sidenote">Technically, the cosine similarity between two vectors is representative of the semantic similarity between those two words in the context of the corpus that the model was trained on.</span>This model has a few shortcoming, though. From a performance perspective, it is over 6 gb and takes several minutes to load into RAM on my computer. Additionally, the model is from 2013, so many recent developments such as COVID-19 and BLM are not part of the model. </p>

        <p>In order to represent a news article as a set of numbers, I decided to create a "document vector" for each article. I approximated a <a href="https://arxiv.org/abs/1405.4053">doc2vec</a> approach, by using the previous word2vec model to generate a 300-dimensional vector for each word in each article. I then combined these vectors all words in an article, weighted by the frequency of those words. In the end, each article is represented by a 300-dimensional vector. </p>

        <p>It is important to note that these models are "bag-of-word" approaches. This means that they consider each news article as a collection of terms. There is no analysis of the relationship between words or the context that the words are used. While this is different from how you and I might assess bias of an article, this "bag-of-word" approach is common in NLP and has the added bonus of being computationally efficient.</p>

        <p>Repeating the aforementioned steps for all 2,000+ articles, we can fill up a 3,000 dimensional vector space with 2,000+ points. This vector space is obviously impossible to properly visualize as it is more than 3-dimensions; however, I can use <a href="https://en.wikipedia.org/wiki/Principal_component_analysis"> principal component analysis (PCA)</a> as a dimensionality reduction technique. This algorithm allows us to extract the two orthogonal basis vectors that explain the most variance in the dataset.</p>

        <p> We can plot these the data in terms of these two orthogonal basis vectors and color each point by the political lean of the article. Red points represent articles collected from right-leaning subreddits, and blue points represent articles collected from left-leaning subreddits.</p>

        <figure>
            <label for="pca" class="margin-toggle">&#8853;</label><input type="checkbox" id="pca" class="margin-toggle"/><span class="marginnote">Principal Components 1 and 2</span>
            <img src="PCA.png" alt="PCA" />
          </figure>
        
        <p>It is disappointing that there is not a clear separation between the blue and red points. This indicates that our classification model will have a difficult time distinguishing between the two types of articles. However, if we look at the plot below, we can see that the first two principal components make up a very small percent of the overall variance in the dataset. Our classification model will therefore likely use many more than 2 out of 300 features.</p>

        <figure>
        <label for="pca2" class="margin-toggle">&#8853;</label><input type="checkbox" id="pca2" class="margin-toggle"/><span class="marginnote">Explained Variance by Principal Components</span>
        <img src="PCA2.png" alt="PCA2" />
        </figure>
      </section>

      <section>
        <h2 id="classification">Building a Binary Classifier</h2>
        <div class="epigraph">
          <blockquote>
            <p>The English language . . . becomes ugly and inaccurate because our thoughts are foolish, but the slovenliness of our language makes it easier for us to have foolish thoughts.</p>
            <footer>George Orwell, “Politics and the English Language”</footer>
          </blockquote>
          <blockquote>
            <p>For a successful technology, reality must take precedence over public relations, for Nature cannot be fooled.</p>
            <footer>Richard P. Feynman, <cite>“What Do You Care What Other People Think?”</cite></footer>
          </blockquote>
          <blockquote>I do not paint things, I paint only the differences between things.<footer>Henri Matisse, <cite>Henri Matisse Dessins: thèmes et variations</cite> (Paris, 1943), 37</footer></blockquote>
        </div>
        <p>If you’d like to introduce your page or a section of your page with some quotes, use epigraphs. Modeled after chapter epigraphs in Tufte’s books (particularly <em>Beautiful Evidence</em>), these are <code>blockquote</code> elements with a bit of specialized styling. Quoted text is italicized. The source goes in a <code>footer</code> element inside the <code>blockquote</code>. We have provided three examples in the epigraph of this section, demonstrating shorter and longer quotes, with and without a paragraph tag, and showing how multiple quotes within an epigraph fit together with the use of a wrapper class.</p>
      </section>

      <section>
        <h2 id="sidenotes">Sidenotes: Footnotes and Marginal Notes</h2>
        <p>One of the most distinctive features of Tufte’s style is his extensive use of sidenotes.<label for="sn-extensive-use-of-sidenotes" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-extensive-use-of-sidenotes" class="margin-toggle"/><span class="sidenote">This is a sidenote.</span> Sidenotes are like footnotes, except they don’t force the reader to jump their eye to the bottom of the page, but instead display off to the side in the margin. Perhaps you have noticed their use in this document already. You are very astute.</p>
        <p>Sidenotes are a great example of the web not being like print. On sufficiently large viewports, Tufte CSS uses the margin for sidenotes, margin notes, and small figures. On smaller viewports, elements that would go in the margin are hidden until the user toggles them into view. The goal is to present related but not necessary information such as asides or citations <em>as close as possible</em> to the text that references them. At the same time, this secondary information should stay out of the way of the eye, not interfering with the progression of ideas in the main text.</p>
        <p>Sidenotes consist of two elements: a superscript reference number that goes inline with the text, and a sidenote with content. To add the former, just put a label and dummy checkbox into the text where you want the reference to go, like so:</p>
        <pre><code>&lt;label for="sn-demo"
       class="margin-toggle sidenote-number"&gt;
&lt;/label&gt;
&lt;input type="checkbox"
       id="sn-demo"
       class="margin-toggle"/&gt;</code></pre>
        <p>You must manually assign a reference <code>id</code> to each side or margin note, replacing “sn-demo” in the <code>for</code> and the <code>id</code> attribute values with an appropriate descriptor. It is useful to use prefixes like <code>sn-</code> for sidenotes and <code>mn-</code> for margin notes.</p>
        <p>Immediately adjacent to that sidenote reference in the main text goes the sidenote content itself, in a <code>span</code> with class <code>sidenote</code>. This tag is also inserted directly in the middle of the body text, but is either pushed into the margin or hidden by default. Make sure to position your sidenotes correctly by keeping the sidenote-number label close to the sidenote itself.</p>
        <p>If you want a sidenote without footnote-style numberings, then you want a margin note.
          <label for="mn-demo" class="margin-toggle">&#8853;</label>
          <input type="checkbox" id="mn-demo" class="margin-toggle"/>
          <span class="marginnote">
            This is a margin note. Notice there isn’t a number preceding the note.
          </span> On large screens, a margin note is just a sidenote that omits the reference number. This lessens the distracting effect taking away from the flow of the main text, but can increase the cognitive load of matching a margin note to its referent text. However, on small screens, a margin note is like a sidenote except its viewability-toggle is a symbol rather than a reference number. This document currently uses the symbol &#8853; (<code>&amp;#8853;</code>), but it’s up to you.</p>
        <p>Margin notes are created just like sidenotes, but with the <code>marginnote</code> class for the content and the <code>margin-toggle</code> class for the label and dummy checkbox. For instance, here is the code for the margin note used in the previous paragraph:</p>
        <pre><code>&lt;label for="mn-demo" class="margin-toggle"&gt;&amp;#8853;&lt;/label&gt;
&lt;input type="checkbox" id="mn-demo" class="margin-toggle"/&gt;
&lt;span class="marginnote"&gt;
  This is a margin note. Notice there isn’t a number preceding the note.
&lt;/span&gt;</code></pre>
        <p>Figures in the margin are created as margin notes, as demonstrated in the next section.</p>
      </section>

      <section>
        <h2 id="figures">Figures</h2>
        <p>Tufte emphasizes tight integration of graphics with text. Data, graphs, and figures are kept with the text that discusses them. In print, this means they are not relegated to a separate page. On the web, that means readability of graphics and their accompanying text without extra clicks, tab-switching, or scrolling.</p>
        <p>Figures should try to use the <code>figure</code> element, which by default are constrained to the main column. Don’t wrap figures in a paragraph tag. Any label or margin note goes in a regular margin note inside the figure. For example, most of the time one should introduce a figure directly into the main flow of discussion, like so:</p>
        <figure>
          <label for="mn-exports-imports" class="margin-toggle">&#8853;</label><input type="checkbox" id="mn-exports-imports" class="margin-toggle"/><span class="marginnote">From Edward Tufte, <em>Visual Display of Quantitative Information</em>, page 92.</span>
          <img src="img/exports-imports.png" alt="Exports and Imports to and from Denmark & Norway from 1700 to 1780" />
        </figure>

        <p><label for="mn-figure-1" class="margin-toggle">&#8853;</label><input type="checkbox" id="mn-figure-1" class="margin-toggle"/><span class="marginnote"><img src="img/rhino.png" alt="Image of a Rhinoceros"/>F.J. Cole, “The History of Albrecht Dürer’s Rhinoceros in Zooological Literature,” <em>Science, Medicine, and History: Essays on the Evolution of Scientific Thought and Medical Practice</em> (London, 1953), ed. E. Ashworth Underwood, 337-356. From page 71 of Edward Tufte’s <em>Visual Explanations</em>.</span> But tight integration of graphics with text is central to Tufte’s work even when those graphics are ancillary to the main body of a text. In many of those cases, a margin figure may be most appropriate. To place figures in the margin, just wrap an image (or whatever) in a margin note inside a <code>p</code> tag, as seen to the right of this paragraph.</p>
        <p>If you need a full-width figure, give it the <code>fullwidth</code> class. Make sure that’s inside an <code>article</code>, and it will take up (almost) the full width of the screen. This approach is demonstrated below using Edward Tufte’s English translation of the Napoleon’s March data visualization. From <em>Beautiful Evidence</em>, page 122-124.</p>
        <figure class="fullwidth">
          <img src="img/napoleons-march.png" alt="Figurative map of the successive losses of the French Army in the Russian campaign, 1812-1813" />
        </figure>
        <p>One obstacle to creating elegant figures on the web is the difficulty of handling different screen sizes, especially on the fly. Embedded <code>iframe</code> elements are particularly troublesome. For these instances we provide a helper class, <code>iframe-wrapper</code>, the most common use for which is probably YouTube videos, e.g.</p>
        <pre><code>&lt;figure class="iframe-wrapper"&gt;
  &lt;iframe width="853" height="480" src="https://www.youtube.com/embed/YslQ2625TR4" frameborder="0" allowfullscreen&gt;&lt;/iframe&gt;
&lt;/figure&gt;</code></pre>
        <figure class="iframe-wrapper">
          <iframe width="853" height="480" src="https://www.youtube.com/embed/YslQ2625TR4" frameborder="0" allowfullscreen></iframe>
        </figure>
        <p>You can use this class on a <code>div</code> instead of a <code>figure</code>, with slightly different results but the same general effect. Experiment and choose depending on your application.</p>
      </section>

      <section>
        <h2 id="code">Code</h2>
        <p>Technical jargon, programming language terms, and code samples are denoted with the <code>code</code> class, as I’ve been using in this document to denote HTML. Code needs to be monospace for formatting purposes and to aid in code analysis, but it must maintain its readability. To those ends, Tufte CSS follows GitHub’s font selection, which shifts gracefully along the monospace spectrum from the elegant but rare Consolas all the way to good old reliable Courier.</p>
        <p>Extended code examples should live in a <code>code</code> element within a <code>pre</code> element. This adds control over indentation and overflow as well:</p>
        <pre><code>;; Some code examples in Clojure. This is a comment.

;; applying a function to every item in the collection
(map tufte-css blog-posts)
;;;; if unfamiliar, see http://www.lispcast.com/annotated-map

;; side-effecty loop (unformatted, causing text overflow) - from https://clojuredocs.org/clojure.core/doseq
(doseq [[[a b] [c d]] (map list (sorted-map :1 1 :2 2) (sorted-map :3 3 :4 4))] (prn (* b d)))

;; that same side-effecty loop, formatted
(doseq [[[a b] [c d]] (map list
                           (sorted-map :1 1 :2 2)
                           (sorted-map :3 3 :4 4))]
  (prn (* b d)))

;; If this proselytizing has worked, check out:
;; http://howistart.org/posts/clojure/1</code></pre>
      </section>

      <section>
        <h2 id="imagequilts">ImageQuilts</h2>
        <p>Tufte CSS provides support for Edward Tufte and Adam Schwartz’s <a href="http://imagequilts.com/">ImageQuilts</a>. See the <a href="http://www.edwardtufte.com/bboard/q-and-a-fetch-msg?msg_id=0003wk">ET forum announcement thread</a> for more on quilts. Some have ragged edges, others straight. Include these images just as you would any other <code>figure</code>.</p>
        <p>This is an ImageQuilt surveying Chinese calligraphy, placed in a full-width figure to accomodate its girth:</p>
        <figure class="fullwidth"><img src="img/imagequilt-chinese-calligraphy.png" alt="Image of Chinese Calligraphy"/></figure>
        <p>Here is an ImageQuilt of 47 animal sounds over and over, in a figure constrained to the main text region. This quilt has ragged edges, but the image itself is of course still rectangular.</p>
        <figure><img src="img/imagequilt-animal-sounds.png" alt="Image of animal sounds"/></figure>
      </section>

      <section>
        <h2 id="epilogue">Epilogue</h2>
        <p>Many thanks go to Edward Tufte for leading the way with his work. It is only through his kind and careful editing that this project accomplishes what it does. All errors of implementation are of course mine.</p>
      </section>
    </article>
  </body>
</html>

<!-- https://youtu.be/tgpTd4ocXXg -->